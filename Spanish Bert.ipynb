{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccdb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae79a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-serving-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4d1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (\n",
    "    TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    ")\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torchsummary import summary\n",
    "from transformers import BertTokenizer, BertConfig, BertTokenizer\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers import AdamW, BertModel, BertForSequenceClassification\n",
    "from autocorrect import Speller\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b614664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_train():\n",
    "    data = pd.read_csv('raw_data/labels_racism.csv', sep='|')\n",
    "    return data\n",
    "\n",
    "def open_test(test_data_type='sample'):\n",
    "    data = pd.read_csv('raw_data/evaluation_{}.csv'.format(test_data_type), sep='|')\n",
    "    return data\n",
    "\n",
    "def cleaned_df(df):\n",
    "    field = 'message'\n",
    "    df = df.copy()\n",
    "    df[field] = df[field].str.lower()\n",
    "    df[field] = df[field].str.strip(\"\\t\")\n",
    "    df[field] = df[field].str.replace(\"r[^A-Za-z()]\", \" \")\n",
    "    df[field] = df[field].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "    df[field] = df[field].str.replace('\\n', '')\n",
    "    df[field] = df[field].str.replace('\\w*\\d\\w*', '')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_short_words(df, short_len=3):\n",
    "    def get_corpus(text_column):\n",
    "        words = []\n",
    "        for i in text_column:\n",
    "            for j in i.split():\n",
    "                words.append(j.strip())\n",
    "        return words\n",
    "    \n",
    "    df = df.copy()\n",
    "    corpus = get_corpus(df['message'])\n",
    "    counter = Counter(corpus)\n",
    "    most_common = counter.most_common(5)\n",
    "    most_common = dict(most_common)\n",
    "    counter.most_common()[::-1]\n",
    "    k=len(counter.most_common())\n",
    "    l=[]\n",
    "    for i in range(k):\n",
    "        if len(counter.most_common()[i][0]) < short_len:\n",
    "            l.append(counter.most_common()[i][0])\n",
    "    \n",
    "    df[\"message\"] = df[\"message\"].apply(lambda row: ' '.join([word for word in row.split() if word not in l]))\n",
    "    return df\n",
    " \n",
    "\n",
    "def prepare_for_bert(df):\n",
    "    sentences = df.message.values\n",
    "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences] \n",
    "    return sentences\n",
    "\n",
    "def prepare_labels(dataset, is_train):\n",
    "    column_name = 'message'\n",
    "    def label_to_score(scores, label):\n",
    "        if label in scores:\n",
    "            return scores[label]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    scores = {\n",
    "        'racist': 1,\n",
    "        'non-racist': 0,\n",
    "        'unknown': 0.5\n",
    "    }\n",
    "    \n",
    "    if is_train:\n",
    "        per_message = dataset.groupby(column_name).label.apply(list).reset_index()\n",
    "        per_message['label'] = per_message.label.apply(\n",
    "            lambda x: np.round(np.mean([scores[i] for i in x])).astype(\"int64\") \n",
    "        )\n",
    "        return per_message[per_message.label != 0.5].copy()\n",
    "    else:\n",
    "        dataset = dataset.copy()\n",
    "        dataset['label'] = dataset.label.apply(lambda x: label_to_score(scores, x))\n",
    "        return dataset\n",
    "\n",
    "\n",
    "def tokenized_to_trf_inputs(tokenizer, tokenized_sentences):\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_sentences]\n",
    "    MAX_LEN = 258\n",
    "\n",
    "    padding = lambda texts: pad_sequences(texts, \n",
    "                                        maxlen=MAX_LEN, \n",
    "                                        dtype=\"long\", \n",
    "                                        truncating=\"post\", \n",
    "                                        padding=\"post\"\n",
    "                                        )\n",
    "\n",
    "\n",
    "    input_ids = padding(input_ids)\n",
    "    get_attention_masks = lambda input_ids: [[float(i>0) for i in seq] for seq in input_ids]\n",
    "\n",
    "    attention_masks = get_attention_masks(input_ids)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "def split_train_test(input_ids, attention_masks, labels):\n",
    "    X_train, X_test, mask_train, mask_test, y_train, y_test = train_test_split(input_ids, \n",
    "                                                                           attention_masks, \n",
    "                                                                           labels, \n",
    "                                                                           test_size=0.3)\n",
    "    return X_train, X_test, mask_train, mask_test, y_train, y_test\n",
    "\n",
    "\n",
    "def split_and_tensorize(input_ids, attention_masks, labels, batch_size=64):\n",
    "    X_train, X_test, mask_train, mask_test, y_train, y_test = split_train_test(input_ids, attention_masks, labels)\n",
    "    X_train = torch.tensor(X_train)\n",
    "    X_val = torch.tensor(X_val)\n",
    "    X_test = torch.tensor(X_test)\n",
    "\n",
    "    mask_train = torch.tensor(mask_train)\n",
    "    mask_val = torch.tensor(mask_val)\n",
    "    mask_test = torch.tensor(mask_test)\n",
    "\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_val = torch.tensor(y_val)\n",
    "    y_test = torch.tensor(y_test)\n",
    "\n",
    "    train_data = TensorDataset(X_train, mask_train, y_train)\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    val_data = TensorDataset(X_val, mask_val, y_val)\n",
    "    val_dataloader = DataLoader(val_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    test_data = TensorDataset(X_test, mask_test, y_test)\n",
    "    test_dataloader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    return train_data, train_dataloader, val_data, val_dataloader, test_data, test_dataloader\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer, \n",
    "    train_batch_gen,\n",
    "    val_batch_gen,\n",
    "    num_epochs\n",
    "):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True)\n",
    "\n",
    "        for X_batch, mask_batch, y_batch in train_batch_gen:\n",
    "            \n",
    "            model_output = model(X_batch, mask_batch, labels=y_batch)\n",
    "            loss = model_output.loss\n",
    "            logits = model_output.logits\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.train(False)\n",
    "\n",
    "     \n",
    "        for X_batch, mask_batch, y_batch in val_batch_gen:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output = model(X_batch, mask_batch, labels=y_batch)\n",
    "                loss = model_output.loss\n",
    "                logits = model_output.logits\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    test_preds, test_labels = [], []\n",
    "\n",
    "    for X_batch, mask_batch, y_batch in test_dataloader:\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_batch, mask_batch).logits          \n",
    "\n",
    "        y_pred = logits.max(1)[1].detach().gpu().numpy()   \n",
    "        test_preds.extend(y_pred)\n",
    "        test_labels.extend(y_batch.numpy())\n",
    "        \n",
    "        print('F1_score: {0:.2f}%, Accuracy: {1:.2f}%, Precision: {2:.2f}%, Recall: {3:.2f}%'.format(\n",
    "            f1_score(test_labels, test_preds),\n",
    "            accuracy_score(test_labels, test_preds),\n",
    "            precision_score(test_labels, test_preds),\n",
    "            recall_score(test_labels, test_preds)\n",
    "        ))\n",
    "        \n",
    "        return test_preds, test_labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_type = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "\n",
    "data = open_train()\n",
    "clean_data = (\n",
    "    data\n",
    "    .pipe(cleaned_df)\n",
    "    .pipe(remove_short_words)\n",
    "\n",
    ")\n",
    "df=clean_data\n",
    "sentences = prepare_for_bert(df)\n",
    "labels = prepare_labels(df, is_train=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_type)\n",
    "tokenized_sentences = tokenize(sentences, tokenizer)\n",
    "input_ids, attention_masks = tokenized_to_trf_inputs(tokenizer, tokenized_sentences)\n",
    "train_data, train_dataloader, val_data, val_dataloader, test_data, test_dataloader = \\\n",
    "    split_and_tensorize(input_ids, attention_masks, labels, batch_size)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    transformer_type, num_labels=2\n",
    ")\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "model, history = train(\n",
    "    model,\n",
    "    optimizer, \n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    num_epochs\n",
    ")\n",
    "test_preds, test_labels = evaluate_model(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
